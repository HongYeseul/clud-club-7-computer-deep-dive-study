# 5장 작은 것으로 큰 성과 이루기, 캐시  360p ~ 399p

## 캐시란 무엇인가?
1. CPU와 메모리의 속도차
CPU 성능은 매우 빠르지만, 메모리는 상대적으로 느림

시스템 성능은 가장 느린 구성 요소에 의해 제한됨 → 병목 현상

이를 해결하기 위해 CPU와 메모리 사이에 캐시(cache) 계층을 둠

2. 캐시 계층 구조
최신 x86 CPU는 L1(4클럭), L2(10클럭), L3(50클럭) 3단계 캐시 구조를 가짐

캐시는 용량은 작지만 CPU에 거의 필적할 정도로 빠름 → 명령 실행 속도 향상

3. 캐시 불일치 문제
Write-through (연속기입)
캐시와 메모리에 동시에 갱신

일관성 유지 용이하지만 느림 (동기 방식)

Write-back (후기입)
캐시에만 먼저 기록, 메모리는 나중에 갱신

성능 좋지만 불일치 위험 존재 (비동기 방식)

## 메모리는 디스크의 캐시다
디스크는 메모리보다 훨씬 느림 → 메모리를 디스크의 캐시로 사용

리눅스는 여유 메모리를 페이지 캐시로 활용

sync, flush 명령으로 캐시 내용을 디스크와 동기화

최근에는 2TB 이상의 메모리를 장착한 인스턴스를 활용해 디스크 대신 메모리를 스토리지처럼 활용하기도 함 (예: AWS)

## 캐시 친화적인 프로그램 작성법

1. 지역성(Locality)
시간 지역성: 최근 접근한 데이터에 다시 접근할 확률 높음

공간 지역성: 인접한 데이터에 접근할 가능성 높음

→ 캐시는 이 두 지역성을 활용해 성능을 높임

2. 핫 데이터 vs 콜드 데이터
자주 접근되는 데이터(hot)는 캐시 적중률 높이도록 구조상 앞에 배치

자주 안 쓰이는 데이터(cold)는 따로 분리하여 구조체 크기 축소 및 캐시 효율성 향상

구조체 크기 최적화를 통해 한 캐시 라인에 더 많은 노드가 들어오도록 하여 순차 접근 성능 향상

3. 메모리 풀
동적 할당보다 미리 큰 메모리 블록을 확보해 사용하는 방식

공간 지역성 확보 및 파편화 방지

4. 분석 우선
성능 병목이 정말 캐시 때문인지 확인이 중요

도구(profiler) 없이 맹목적으로 캐시 최적화하는 건 비효율적

## 멀티 스레드에서의 캐시 이슈
1. Cache Ping-Pong (Cache Line Bouncing)
여러 CPU 코어가 동일한 메모리 주소를 번갈아 갱신할 때 발생

캐시 일관성 유지 위해 서로 캐시 무효화(invalidate) 반복 → 성능 급락

스레드 수를 늘릴수록 반드시 성능이 증가하지는 않음!

2. False Sharing (거짓 공유)
각기 다른 변수가 같은 캐시 라인에 존재하면, 변수 간에 독립적이어도 불필요한 캐시 충돌 발생

해결: 패딩 추가 또는 구조체 정렬 조정으로 캐시 라인 분리

3. 해결 방법 요약
- 공유 변수 최소화
- 읽기 전용 데이터만 공유
- 캐시 라인 단위로 데이터 분리

## 명령어 순서, 캐시, 그리고 메모리 장벽
1. 명령어의 비순차 실행 (Out-of-Order Execution)
CPU는 성능 향상을 위해 명령어를 작성 순서와 다르게 실행

단일 스레드에선 문제 없음, 멀티 스레드에선 위험

2. 저장 버퍼와 캐시
CPU는 메모리 갱신 요청을 버퍼에 먼저 기록 후 진행 → 외부에서는 아직 변경사항을 모를 수 있음

3. 해결책: 메모리 장벽 (Memory Barrier)
명령어 재배치를 막고 정확한 실행 순서 보장

Load-Load, Store-Store 등 다양한 유형 존재
