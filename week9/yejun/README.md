## 작은것으로 큰 성과 이루기, 캐시
___

폰 노이만 구조에서, CPU가 실행하는 기계 명령어와 그 명령어가 사용하는 데이터가 모두 메모리에 저장되어있어야 한다.
다시 말해, CPU는 빈번하게 메모리에 접근해 명령어와 데이터를 가져와야 하고, 실행 결과를 메모리에 기록해야 한다.

문제는 속도.
CPU의 연산속도보다 메모리의 접근 속도가 훨씬 느리다. 이러면 CPU의 빠른 연산을 충분히 활용할 수 없다.
-> CPU가 직접 메모리를 읽고 쓰지 않도록 하는 것. 중간에 캐시 계층을 둬 해결할 수 있다.

### 캐시, 어디에나 존재하는 것

- 캐시는 CPU가 사용가능할 만큼 속도가 매우 빠르지만 용량이 제한되어 있다
- 캐시 안에는 최근에 메모리에서 얻은 데이터가 저장된다
- CPU는 명령어/데이터를 꺼내와야 할 때 캐시를 먼저 조회해서 적중하면 메모리에 접근하지 않는다

CPU는 메모리에 직접 읽고 쓰지 않음으로써 메모리와의 속도 차이를 보완한다.

x86 아키텍처에서는 L1, L2, L3 의 캐시 계층이 있다.

<img width="429" alt="스크린샷 2025-05-12 오후 10 02 52" src="https://github.com/user-attachments/assets/4a65b8e3-0936-47e9-bf02-42031d0fc034" />

L1은 4클럭, L2는 10클럭, L3는 50클럭 정도의 주기가 소요된다.
 - 각 명령어는 일정한 클럭(CPU가 한 동작을 수행하는 데 걸리는 기본 단위 시간) 수로 실행된다
 - RISC: 명령어 1개당 1클럭 (이상적 모델)
 - CISC: 명령어 1개에 여러 클럭이 걸릴 수 있음

L1, L2, L3 그리고 CPU 코어는 레지스터 칩 내에 묶여 `패키징`되어있다.
오늘날 CPU 칩에서 상당 부분의 공간을 캐시가 차지한다.

이렇게 캐시 계층을 두면 데이터 조회 성능이 개선되지만, 데이터 기록 시에는 갱신 문제가 발생한다.

- 캐시의 데이터와 메모리의 데이터가 inconsistent(불일치)하는 경우 => 해결 방식은 동기식 설계와 비동기식 설계가 있다.
  - 동기식 - write-through : 캐시를 갱신할 때 메모리도 갱신하는 방법. 불필요한 대기시간 발생.
  - 비동기식 - write-back : 일단 캐시만 갱신하고, 용량 부족으로 캐시가 해제될 때가 되면 그 캐시에 발생했던 수정사항을 메모리에 갱신.
- 다중 코어의 경우, 캐시-메모리 간의 갱신 뿐만 아니라 각 프로세서 캐시들 간의 일관성도 유지되어야 한다.
  - 최신 CPU에는 고전적인 MESI (Flag를 두어 상태를 체크) protocol 같은 것으로 다중 코어의 캐시 일관성을 유지한다. 물론 규칙 적용을 위해서는 성능에 대가가 따른다.
    - Modified(수정) 상태 : 데이터가 수정된 상태
    - Exclusive(배타) 상태 : 유일한 복사본이며, 주기억장치의 내용과 동일한 상태
    - Shared(공유) 상태 : 데이터가 두 개 이상의 프로세서 캐시에 적재되어 있는 상태
    - Invalid(무효) 상태 : 데이터가 다른 프로세스에 의해 수정되어 무효화된 상태

___

메모리와 디스크 간의 속도 차이도 존재한다. 메모리 접근 속도는 디스크 탐색 속도보다 `10만배` 가량 빠르다.

따라서 `메모리 자체를 디스크의 캐시`로 사용한다.
(따로 계층을 추가하지 않는 이유는 메모리의 용량이 캐시로 작용하기에 충분하기 때문)

일반적으로 메모리의 사용률은 100%에 도달하지 않기 때문에, 여유 공간을 디스크를 위한 캐시로 활용한다. 이것이 리눅스에서 `페이지 캐시`의 기본 원리.

여기에도 당연히 `갱신 문제` 즉 `inconsistency` 문제가 발생한다. 이를 위해 여러 입출력 라이브러리들은 `sync`, `flush` 같은 함수를 제공함으로써 `inconsistency`가 발생하기 전에 미리 데이터를 갱신할 방법을 제공한다.

메모리의 가격이 저렴해지면서, 서버에서는 메모리가 디스크를 대체하는 흐름이 있다.

(AWS 에서 2TB 짜리 메모리를 제공하는 instance를 제공)

하지만 이는 "용량 때문에 디스크에 의존해야 했던 상황"을 해결한 것이고, 영속성에 대한 의존은 남아있기 때문에 완전한 대체는 불가능하다.

가상메모리에서 swap-in/out 은 디스크를 일종의 '창고'처럼 활용하는 방법.

<img width="675" alt="스크린샷 2025-05-12 오후 10 27 08" src="https://github.com/user-attachments/assets/0aa95765-c204-4e90-939c-12ab5b4c92bd" />

결국 CPU가 메모리에서 데이터를 읽는 방법은 아래와 같이 정리가 가능하다.

1. 가상 메모리 주소를 실제 물리 주소로 변환.
2. L1, L2, L3 캐시를 검색해서 찾았다면 바로 반환.
3. 못찾았다면 메모리에서 찾아서 반환.
4. 메모리에서도 못찾고 디스크로 swap-out 된 상태이면 디스크에 접근해서 다시 메모리로 적재 후 반환

___
```
분산 파일 시스템 (distributed file system)
대용량 데이터 저장 문제를 해결하는 방법 중 하나로, 데이터를 여러 장치에 나누어 저장하는 것.
```
<img width="461" alt="스크린샷 2025-05-12 오후 10 31 08" src="https://github.com/user-attachments/assets/d18c27c8-3047-493a-8fb8-ae97cece3482" />

원격으로 저장되는 경우에, 원격 분산 파일 시스템에서 전송된 데이터를 로컬 디스크가 저장
이때 로컬 디스크를 `원격 분산 파일 시스템의 캐시`로 볼 수 있음.

더불어, 원격 분산 파일 시스템의 데이터를 data stream 의 형태로 로컬 컴퓨터의 메모리에 끌어올 수도 있음. (apache kafka)
이때는 메모리가 `원격 분산 파일 시스템의 캐시`로 간주됨.

결론: 컴퓨터의 저장 체계의 각 계층이 다음 계층에 대한 캐시 역할을 한다.

단, 각 계층의 저장 용량은 다음 계층의 저장 용량보다 작아야 한다 (그렇지 않으면 그 계층으로 다음 계층을 대체하면 됨)

**컴퓨터 전체 저장 체계가 최대의 성능을 발휘하기 위해선 캐시 친화적인 프로그램을 작성하는 것이 매우 중요.**

___

### 어떻게 캐시 친화적인 프로그램을 작성할까?

어떤 프로그램이 캐시 친화적이라는 것은, 그 프로그램의 캐시의 적중률이 높다는 것이다.

```
locality (지역성)
( locality of reference / principle of locality )
프로그램이 "매우 규칙적으로" 메모리에 접근하는 경향성이 있다는 것.
```
- temporal locality : 특정 메모리 조각을 여러번 참조하는 경향. (시간적 지역성)
- spatial locality : 참조한 메모리에 인접한 메모리를 참조하는 경향. (공간적 지역성)

이러한 경향성은 캐시 친화적이다

캐시 친화적인 프로그래밍 원칙 몇 가지.

1. 메모리 풀 사용.

메모리 풀은 커다란 메모리 조각을 한 번에 할당받아서 나눠쓴다. 결국 그 커다란 메모리 조각을 할당 받는 것이 곧 연속적인 메모리 공간을 할당 받는 것.
spatial locality가 우수하다.

2. struct 구조체 최적화.

같이 빈번하게 사용되는 멤버들을 서로 인접하게 배치하는 것으로 spatial locality를 높일 수 있다.

빈번하게 접근하는 데이터를 `hot data`, 거의 접근하지 않는 데이터를 `cold data`라고 할 때

hot data와 cold data를 서로 분리하면 더 나은 지역성을 얻을 수 있다.
```
예를 들어, 구조체 멤버로 잘 접근하지 않는 큰 배열이 있다면 포인터만 멤버로 갖고 배열 자체는 구조체와 분리하는 것.
```

3. 데이터 구조

순전히 지역성의 관점에서 보면, 배열이 연결 리스트보다 낫다.

배열은 하나의 연속된 공간에 할당되는 반면, 연결 리스트는 일반적으로 각 노드들이 메모리에 흩어져있을 가능성이 높기 때문.

연결 리스트를 직접 정의한 메모리 풀에서 구성한다면 단점 보완 가능

4. 코드 탐색 패턴

다차원 배열이 있을 때 '행 우선 방식'으로 데이터가 메모리에 저장된다면(첫 번째 행의 모든 데이터를 쭉 저장하고 그 다음 두 번째 행의 모든 데이터를 저장하는 형식)

이중 반복문에서 바깥 반복문이 `행 순회`, 안쪽 반복문이 `열 순회` 일때 캐시 적중률이 높음(순회 기준이 반대라면 캐시 적중률이 매우 떨어질 것)

```
결론은 코드 탐색 패턴의 약간의 차이만 생겨도 캐시 적중률의 차이가 현저할 수 있다.
이 원칙을 적용할 때 중요한 점은 캐시 적중률로 인한 병목이 있을 때만 이런 최적화를 진행하라는 것
```
___

### 다중 스레드 성능 방해자

프로그램이 가지는 spatial locality 에 따라, 접근하고자 하는 데이터와 인접한 데이터까지 `묶음`으로 캐시에 저장하는 것이 캐시 친화적이다.

이 `묶음`을 캐시 라인(cache line)이라고 한다.

캐시 라인의 크기는 일반적으로 64바이트이며, 캐시가 적중하지 못할 경우 그 데이터가 속한 캐시 라인이 캐시에 저장된다 (인접한 메모리에 있는 데이터를 Hit할 확률이 높기 때문)

**하지만 캐시 라인 단위의 상호작용은 다중 스레드에서 성능을 방해하는 요인이 되기도 한다.**

___

<img width="690" alt="스크린샷 2025-05-14 오후 8 20 46" src="https://github.com/user-attachments/assets/f02dff10-c116-486d-9234-2dabdfa85ff5" />

다중 코어를 사용하는 다중 스레드가 하나의 데이터를 공유한다면, 코어 별로 캐시를 가지고 있으므로 캐시 일관성을 위해 한 쪽이 데이터를 변경할 때 다른 한 쪽의 캐시를 무효화(invalidation)해야 한다.

이는 결국 서로가 서로의 캐시를 끊임없이 무효화 하는 현상으로 이어지고, 이러한 현상을 일컬어 **"cache line bouncing"** 또는 **"cache ping-pong"** 이라고 한다.

이 현상이 발생하면 캐시 본연의 기능을 활용하지 못하고 전체 흐름이 방해되면서 프로그램 성능이 크게 저하된다.

<img width="690" alt="스크린샷 2025-05-14 오후 8 19 02" src="https://github.com/user-attachments/assets/6ad4640d-30cc-4f25-b043-11ee7dc084a9" />

다중 코어를 사용하는 다중 스레드가, 같은 데이터가 아니라 **"서로 다르지만 같은 캐시 라인에 있는 데이터"에 각각 쓰기를 하려고 하더라도 cache line bouncing 현상이 발생**한다.

데이터를 공유하지 않더라도 캐시 라인을 공유하면 문제가 된다. 왜냐하면 캐시 일관성을 유지하는 입장에서 캐시 안에 다른 코어와 공유하고 있는 데이터가 있고, (해당 코어가 사용하지 않는 데이터라고 하더라도)

그 데이터에 수정이 발생했다면 동기화 해주는 것이 안전하기 때문. 이는 **"false sharing"**이라고 하고, 이 또한 cache line bouncing 문제를 야기한다.

**해결법은 false sharing 하고 있는 데이터들이 같은 캐시 라인(64Bytes)에 속하지 않도록 데이터에 padding**을 주는 것. 구조체를 예로 들면 멤버 순서를 바꾸거나 중간에 배열을 끼워넣는 식.




