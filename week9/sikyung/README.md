## 5장 작은 것으로 큰 성과 이루기, 캐시

### 5.1. 캐시, 어디에나 존재하는 것

- CPU와 메모리의 속도차이

  - 시스템 성능은 속도가 상대적으로 느린쪽에 맞추어 제한되므로 CPU와 메모리 속도가 같아야 가장 좋은 성능을 발휘함(나무통 원리)
  - 하지만, 현실은 CPU는 매우 배고픈 존재 ~
  - <img width="377" alt="Image" src="https://github.com/user-attachments/assets/4360bbe7-10c6-4451-9676-6c4f348e1cf7" />

- 최신 CPU는 메모리 사이에 캐시 계층이 추가되어 있음. 캐시는 가격이 비싸고 용량이 제한적이지만 접근 속도가 거의 CPU에 필적함 > CPU가 명령어 실행 속도 크게 끌어올림.
- x86 > L1캐시(4클럭), L2캐시(10클럭), L3캐시 존재(50클럭)

  - 3계층 캐시와 CPU코어는 레치스터 칩내에 패키징되어 있음.

- 캐시와 메모리 사이의 불일치 문제

  - 캐시-메모리 일관성
    - 컴퓨터는 속도 차이가 큰 CPU와 메모리 사이에 캐시라는 중간 저장 공간을 둠. 이떄 문제는 캐시에 있는 데이터가 메모리와 불일치할 수 있다는 것. 이 불일치를 해결하는 두 가지 대표 방식이 `write-through`와 `write-back`임
  - `Write-through (연속기입 방식)`: 캐시를 수정할 때마다 바로 메모리에도 같은 내용을 반영함.

    - CPU는 메모리 갱신이 끝날 때까지 기다려야 해서 속도가 느려짐.
    - 캐시와 메모리의 일관성 유지가 쉬움.
    - 동기식 구조와 유사함: 작업이 순차적으로 진행됨.

  - `Write-back (후기입 방식)`: 캐시에만 우선 데이터를 쓰고, 메모리는 나중에 필요할 때 갱신.
    - 성능이 우수함: CPU는 캐시만 쓰고 바로 다음 명령 실행 가능.
    - 비동기식 구조와 유사함: 작업이 나중에 병렬 또는 지연 처리됨.
    - 캐시가 꽉 찼을 때, 수정된 데이터를 메모리에 쓸 필요가 있음.

- 메모리를 디스크의 캐시로 활용하기

  - 메모리는 디스크의 캐시

    - 디스크 접근 속도가 느려 메모리 데이터를 디스크의 캐시로 활용함
    - 이를 통해 파일 접근 시 디스크 입출력 없이 메모리에서 직접 가져올 수 있음

  - 여유 메모리 공간의 활용

    - 시스템은 항상 일정 부분의 메모리를 사용하지 않음
    - 남는 메모리 공간을 디스크 캐시로 활용하여 디스크 접근을 최소화함

  - 캐시 갱신 문제 발생

    - 캐시에는 기록되었지만 디스크에 갱신되지 않아 데이터 불일치가 생길 수 있음
    - 충돌이나 정전 시 데이터 유실을 막기 위해 `sync`, `flush` 등을 사용해 동기화를 수행함

  - 메모리가 디스크를 대체하는 추세
    - 메모리 가격이 지속적으로 하락함에 따라 디스크 대신 메모리를 사용하는 방식이 증가함
    - AWS 등에서는 2TB 메모리를 탑재한 인스턴스를 제공하며, 이는 저장소로 메모리를 활용함

### 5.2. 어떻게 캐시 친화적인 프로그램을 작성할까?

- 핫 데이터와 콜드 데이터의 분리

  - 연결 리스트에서 자주 접근되는 `next`, `value`는 **핫 데이터(hot data)**임
  - 반면, 크고 자주 쓰이지 않는 `arr` 배열은 **콜드 데이터(cold data)**임

- 구조 최적화의 이유

  - 캐시는 용량이 제한되어 있고, 자주 접근되는 데이터만 효과적으로 저장 가능함
  - 구조체 크기가 크면 한 캐시 라인에 들어갈 수 있는 노드 수가 줄어들어 성능이 떨어짐

- 최적화 기법

  - 자주 접근되지 않는 `arr` 배열을 따로 분리된 구조체(`struct Arr`)로 두고,  
    원래 리스트(`struct List`)에서는 포인터로만 참조함
  - 이를 통해 `struct List` 자체의 크기를 줄여 **캐시 효율성**을 높임
  - CPU 캐시는 일반적으로 캐시 라인 단위로 메모리를 가져옴 (예: 64바이트)

    - 리스트 노드가 커지면 한 캐시 라인에 노드가 하나밖에 못 들어와 캐시 미스율 증가
    - 반면 핫 데이터만 묶어두면 한 캐시 라인에 여러 노드가 들어와 순차 접근 시 locality 향상
    - 이런 최적화는 OS 커널, 게임 엔진, 성능 민감한 시스템 소프트웨어 등에서 자주 사용됨
    - 단점: arr를 사용할 때는 간접 참조가 한 번 더 필요하므로 접근이 느릴 수 있음 → 접근 패턴 분석 필수

  - 코드 예시

    ```c
    #define SIZE 100000

    struct List {
        List* next;
        int value;
        struct Arr* arr;
    };

    struct Arr {
        int arr[SIZE];
    };
    ```

- 정리

  - 지역성(Locality)을 이해하기
  - 캐시 적중률이 병목인지 확인하는 것이 우선!

    - 반드시 성능 분석 도구를 통해 캐시 적중률이 시스템 병목인지 판단해야 함
    - 병목이 아니라면 캐시 최적화에 과도하게 집착할 필요 없음

  - 다중 코어 환경에선 병목 요인이 더 다양해짐

    - 다중 스레드 프로그래밍이 일반화되며, 캐시와의 충돌이나 동기화 문제가 발생함
    - 이후에는 다중 스레드 환경에서 주의해야 할 점을 더 알아보게 될 예정임

### 5.3. 다중 스레드 성능 방해자

1. 캐시 튕김(Cache Ping-Pong, Cache Line Bouncing) 문제

- 다중 코어 시스템에서 여러 CPU 코어가 **같은 메모리 주소**를 번갈아 쓰게 되면 발생함
- 코어 간의 캐시가 서로 **일관성을 유지하려고 캐시를 무효화(invalidate)**하면서 계속 데이터를 주고받는 현상임

- 동작 예시

  - C1 캐시가 a 값을 수정하면, C2 캐시의 a는 무효화됨
  - 이후 C2가 a를 수정하면 C1의 캐시가 무효화됨
  - 이 과정이 반복되며 **캐시 간의 불필요한 데이터 이동**이 끊임없이 발생함
  - <img width="355" alt="Image" src="https://github.com/user-attachments/assets/ff08b3d6-90ed-4817-b042-6e59bbada699" />

- 성능 저하 원인

  - 캐시의 장점인 빠른 접근이 사라지고, **매번 메모리에서 새로 데이터를 읽어야 하는 상황**이 발생함
  - 캐시 무효화와 동기화 오버헤드가 커져서, **전체 프로그램 성능이 급격히 저하됨**
  - 멀티스레드 프로그램이 단일 스레드보다 더 느려질 수 있음

- 해결 방향

  - 가능한 한 **여러 스레드 사이에서 데이터를 공유하지 않도록 설계**해야 함
  - 공유가 필요하다면 캐시 라인 단위로 **false sharing**을 피하도록 데이터 배치 구조를 조정해야 함

2. 거짓 공유 문제

### 5.4. 봉화히제후와 메모리 장벽

### 명령어의 비순차적 실행과 캐시, 멀티코어

1. 명령어는 작성 순서대로 실행되지 않음

   - CPU는 프로그램 코드를 작성된 순서 그대로 실행하지 않음
   - 이유: 더 빠르게 실행하려고 컴파일러가 명령어를 재배치하거나 CPU가 순서를 바꿔 실행함 (→ Out of Order Execution, OoOE)

2. 예시로 설명: a = b + 100; b = 200;

   - 이 코드는 작성 순서는 a를 먼저 계산하고 b를 설정하지만
   - CPU는 b = 200을 먼저 실행해도 문제가 없기 때문에 순서를 바꿔 실행할 수 있음

3. 명령어 재정렬이 문제를 일으키는 경우

   - 단일 스레드 안에서는 재정렬이 있어도 결과는 같음
   - 하지만 여러 스레드가 동시에 같은 데이터를 사용할 땐, 이 재정렬이 **문제**를 만듦
   - 한 스레드는 변수 a를 설정 중인데 다른 스레드는 그걸 보고 이미 설정되었다고 착각할 수 있음

4. 저장 버퍼와 캐시의 역할

   - CPU는 어떤 값을 메모리에 ‘기록하겠다’고 하면, 실제로는 저장 버퍼에 먼저 기록함
   - 이 값은 아직 캐시나 메모리에 들어가지 않았지만, CPU는 이미 설정된 것처럼 행동함
   - 다른 CPU 코어는 이 값을 아직 볼 수 없으므로 **혼선 발생 가능**

5. 이로 인해 생기는 문제: 실행 순서가 꼬인다

   - A 코어는 `a = 1` 하고 나서 `b = y`를 실행
   - 그런데 이 a = 1이 아직 캐시에도 안 들어갔는데, B 코어가 b 값을 읽어 가버리면 a 값이 0일 수도 있음
   - 결과적으로 b = y가 먼저 실행된 것처럼 보임 → 순서가 꼬인 듯한 현상 발생

6. 해결책: 메모리 장벽 (Memory Barrier)

   - CPU에게 “이 명령어들 순서 꼭 지켜!”라고 강제로 말하는 명령어
   - 이를 이용하면 재정렬을 막고, **순서 보장**이 가능해짐

### 잠금과 잠금 없는 프로그래밍

1. 잠금(lock)은 공유 데이터를 보호하는 방법

   - 여러 스레드가 동시에 데이터를 바꾸면 꼬일 수 있으니, 먼저 차지한 스레드가 작업을 끝낼 때까지 막아 둠
   - 대표적 방법: mutex, spinlock

2. 잠금 없는 프로그래밍 (lock-free)

   - 공유 데이터에 접근할 때 **굳이 차례 기다릴 필요가 없도록** 만든 프로그래밍 기법
   - 핵심: 한 스레드는 언제든 작업을 계속할 수 있어야 함
   - 사용 예: compare-and-swap(CAS) 알고리즘

3. 잠금 없는 환경에서는 재정렬이 더 민감한 문제

   - 공유 변수의 상태를 여러 스레드가 동시에 확인하기 때문에,
   - 한쪽이 변수 값을 설정했는지 안 했는지 정확히 알아야 함
   - 재정렬이 발생하면, 아직 설정도 안 된 데이터를 읽는 사고 발생 가능

4. 해결책: 메모리 장벽 사용

   - 명령어는 재정렬되더라도, **메모리 접근 순서만은 강제 정렬**해주는 장치
   - Load-Load, Store-Store 등 다양한 장벽이 존재함

### 요약

1. CPU는 코드를 순서대로 실행하지 않음 (성능을 위해 순서 재배치함)
2. 단일 스레드에서는 문제 안 생김
3. 멀티 스레드에서는 이 재정렬이 **심각한 버그**로 이어질 수 있음
4. 이걸 막기 위해선 **메모리 장벽**이 필요함
5. **잠금 없는 프로그래밍**을 할 땐 반드시 이 점을 인지하고 코딩해야 함
